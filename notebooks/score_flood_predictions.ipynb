{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To score flood predictions you must have flood labels available of course. Those should be added to your datafolder in a similar structure to the example in `data/inference_sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from flood_mapping.utils import create_path_df\n",
    "\n",
    "data_folder = Path(\"data/inference_sample\")\n",
    "path_df = create_path_df(data_folder, add_flood_label_paths=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is calculated with IOU score, Intersect Over Union, which in short gives penalties for both types of errors. If you predict a flood at a pixel where there is none, the union gets larger. If you don't predict a flood at a pixel where there is one, the intersect gets smaller. Both result in a lower score compared to the score of 1 if you predict all floods correctly and none incorrectly (intersect=union, IOU=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(label, pred):\n",
    "    intersection = np.logical_and(label, pred)\n",
    "    union = np.logical_or(label, pred)\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "    return(iou_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_scores_never_flood = list()\n",
    "iou_scores = list()\n",
    "iou_scores_postprocessed = list()\n",
    "iou_scores_posthigh = list()\n",
    "never_flood_pred = np.zeros((256, 256))\n",
    "\n",
    "for pred_path, label_path in zip(path_df['pred_path'], path_df['flood_label_path']):\n",
    "    flood_label = cv2.imread(str(label_path), 0) / 255\n",
    "    flood_pred = cv2.imread(str(pred_path), 0) / 255\n",
    "\n",
    "    iou_score = calc_iou(flood_label, flood_pred)\n",
    "    iou_scores.append(iou_score)\n",
    "\n",
    "\n",
    "iou_scores = np.array(iou_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unclear whether the challenge where this nvidia model was taken from counted predicting no floods for an image with no floods as a score of 1 for that image and took the average of all individual IOU scores to get the challenge rating, or that the IOU score is calculated pasting all images together to one big image, ignoring the non flood images. The latter is most commonly how we find it described in literature. Both scores are calculated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{np.nanmean(iou_scores):0.3} - IOU Score ignoring non flood images')\n",
    "print(f'{np.nan_to_num(iou_scores, nan=1).mean():0.3} - IOU Score where correct non flood images count as 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the scores in a bar chart to check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.histogram(np.nan_to_num(iou_scores, nan=1), title='IOU score distribution').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated-flood-extent-mapping-iiypELG_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
